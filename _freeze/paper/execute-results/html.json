{
  "hash": "8b07b2417bdb33b6b9a0a83330d80ab3",
  "result": {
    "markdown": "---\ntitle: \"Assessing the performance of spatial cross-validation approaches for models of spatially structured data\"\nformat:\n  arxiv-pdf:\n    keep-tex: true\n    output-file: spatial_cv_arxiv\n    linenumbers: true\n    runninghead: Assessing spatial cross-validation approaches\n  elsevier-pdf:\n    keep-tex: true\n    include-in-header: \n      text: |\n        \\newpageafter{author}\n    journal: \n      name: \"Environmental Modelling \\\\\\\\& Software\"\n      cite-style: authoryear\n      highlights:\n        - \"Spatial cross-validation produces more accurate estimates of model performance\"\n        - \"We provide a taxonomy and evaluation of common spatial cross-validation methods\"\n        - \"The best methods combined spatially conjunct assessment sets with exclusion buffers\"\n  html: default\neditor: visual\nauthor:\n  - name: Michael J Mahoney\n    affiliations:\n      - ref: GPES\n    orcid: 0000-0003-2402-304X\n    email: mjmahone@esf.edu\n    url: https://mm218.dev\n    correspondence: \"yes\"\n    attributes:\n      corresponding: true\n  - name: Lucas K Johnson\n    affiliations:\n      - ref: GPES\n    orcid: 0000-0002-7953-0260\n    email: ljohns11@esf.edu\n  - name: Julia Silge\n    affiliations:\n      - ref: Posit\n    orcid: 0000-0002-3671-836X\n    email: julia.silge@posit.co\n  - name: Hannah Frick\n    affiliations:\n      - ref: Posit\n    email: hannah@posit.co\n    orcid: 0000-0002-6049-5258\n  - name: Max Kuhn\n    affiliations:\n      - ref: Posit\n    orcid: 0000-0003-2402-136X\n    email: max@posit.co\n  - name: Colin M Beier\n    affiliations:\n      - ref: SRM\n    email: cbeier@esf.edu\n    orcid: 0000-0003-2692-7296\naffiliations:\n  - id: GPES\n    name: State University of New York College of Environmental Science and Forestry\n    department: Graduate Program in Environmental Science\n    address: 1 Forestry Drive\n    city: Syracuse, NY\n    country: USA\n    postal-code: 13210\n  - id: Posit\n    name: Posit, PBC\n    address: 250 Northern Ave\n    city: Boston, MA\n    country: USA\n    postal-code: \"02210\"\n  - id: SRM\n    name: State University of New York College of Environmental Science and Forestry\n    department: Department of Sustainable Resources Management\n    address: 1 Forestry Drive\n    city: Syracuse, NY\n    country: USA\n    postal-code: 13210\nabstract: |\n  Evaluating models fit to data with internal spatial structure requires specific cross-validation (CV) approaches, because randomly selecting assessment data may produce assessment sets that are not truly independent of data used to train the model. Many spatial CV methodologies have been proposed to address this by forcing models to extrapolate spatially when predicting the assessment set. However, to date there exists little guidance on which methods yield the most accurate estimates of model performance.\n  \n  We conducted simulations to compare model performance estimates produced by five common CV methods fit to spatially structured data. We found spatial CV approaches generally improved upon resubstitution and V-fold CV estimates, particularly when approaches which combined assessment sets of spatially conjunct observations with spatial exclusion buffers. To facilitate use of these techniques, we introduce the `spatialsample` package which provides tooling for performing spatial CV as part of the broader tidymodels modeling framework.\nbibliography: paper.bib\nkeywords:\n  - cross-validation\n  - spatial data\n  - machine learning\n  - random forests\n  - simulation\nnumbersections: true\ndate: \"2023-03-13\"\neditor_options:\n  markdown:\n    wrap: 80\n    canonical: true\n---\n\n\n\n\n\n\n# Introduction {#sec-introduction}\n\nEvaluating predictive models fit using data with internal spatial dependence\nstructures, as is common for Earth science and environmental data\n[@legendre1989], is a difficult task. Low-bias models, such as the machine\nlearning techniques gaining traction across the literature, may overfit on the\ndata used to train them. As a result, model performance metrics may be nearly\nperfect when models are used to predict the data used to train the model (the\n\"resubstitution performance\" of the model) [@Kuhn2013], but deteriorate when\npresented with new data.\n\nIn order to detect a model's failure to generalize to new data, standard\ncross-validation (CV) evaluation approaches assign each observation to one or\nmore \"assessment\" sets, then average performance metrics calculated against each\nassessment set using predictions from models fit using \"analysis\" sets\ncontaining all non-assessment observations. We refer to the assessment set,\nwhich is \"held out\" from the model fitting process, as $D_{\\operatorname{out}}$,\nwhile we refer to the analysis set as $D_{\\operatorname{in}}$. Splitting data\nbetween $D_{\\operatorname{out}}$ and $D_{\\operatorname{in}}$ is typically done\nrandomly, which is sufficient to determine model performance on new, unrelated\nobservations when working with data whose variables are independent and\nidentically distributed. However, when models are fit using variables with\ninternal spatial dependence (often referred to as spatial autocorrelation),\nrandom assignment will likely assign neighboring observations to both\n$D_{\\operatorname{out}}$ and $D_{\\operatorname{in}}$. Given that neighboring\nobservations are often more closely related [@legendre1989], this random\nassignment yields similar results as the resubstitution performance, providing\nover-optimistic validation results that over state the ability of the model to\ngeneralize to new observations or to regions not well-represented in the\ntraining data [@Roberts2017; @bahn2012].\n\nOne potential solution for this problem is to not assign data to\n$D_{\\operatorname{out}}$ purely at random, but to instead section the data into\n\"blocks\" based upon its dependence structure and assign entire blocks to\n$D_{\\operatorname{out}}$ as a unit [@Roberts2017]. For data with spatial\nstructure, this means assigning observations to $D_{\\operatorname{out}}$ and\n$D_{\\operatorname{in}}$ based upon their spatial location, in order to increase\nthe average distance between observations in $D_{\\operatorname{out}}$ and those\nused to train the model. The amount of distance required to ensure accurate\nestimates of model performance is a matter of some debate, with suggestions to\nuse variously the variogram ranges for model predictors, model outcomes, or\nmodel residuals [@lerest2014; @Roberts2017; @telford2009; @valavi2018;\n@karasiak2021]. However, it is broadly agreed that increasing the average\ndistance between observations in $D_{\\operatorname{out}}$ and those used to\ntrain the model may produce more accurate estimates of model performance.\n\nThis objective -- evaluating the performance of a predictive model -- is subtly\nbut importantly distinct from evaluating the accuracy of a map of predictions.\nMap accuracy assessments assume a representative probability sample in order to\nproduce unbiased accuracy estimates [@stehman2019], while assessments of models\nfit using spatial data typically assume a need to estimate model performance\nwithout representative and independent assessment data. Such situations emerge\nfrequently across model-based studies [@degruijter1990; @brus2020], such as\nduring hyperparameter tuning [@schratz2019]; when extrapolating spatially to\npredict into \"unknown space\" without representative assessment data [@meyer2021;\n@meyer2022]; or when working with data collected via non-representative\nconvenience samples (any non-probability sample where observations are collected\non the easiest to access members of a population) as commonly occurs in ecology\nand environmental science [@martin2012; @yates2018]. In these situations,\nunderstanding the predictive accuracy of the model on independent data is\nessential. Although recent research has argued against spatial CV for map\naccuracy assessments [@wadoux2021], spatial CV remains an essential tool for\nevaluating the performance of predictive models.\n\nFor many situations spatial CV has been shown to provide empirically better\nestimates of model performance than non-spatial methods [@bahn2012;\n@schratz2019; @meyer2018; @lerest2014; @ploton2020], and as such is popularly\nused to evaluate applied modeling projects across domains [@townsend2007;\n@meyer2019; @adams2020]. As such, a variety of methods for spatially assigning\ndata to $D_{\\operatorname{out}}$ have been proposed, many of which have been\nshown to improve performance estimates over randomized CV approaches. However,\nthe lack of direct comparisons between alternative spatial CV approaches makes\nit difficult to know which methods may be most effective for estimating model\nperformance. Understanding the different properties of various CV methods is\nparticularly important given that many spatial modeling projects rely on CV for\ntheir primary accuracy assessments [@bastin2019; @fick2017; @vandenhoogen2019;\n@hengl2017].\n\nHere, we evaluated the leading spatial CV approaches found in the literature,\nusing random forest models fit on spatially structured data following the\nsimulation approach of @Roberts2017. To facilitate comparison among methods, we\noffer a useful taxonomy and definition of these CV approaches, attempting to\nunify disparate terminology used throughout the literature. We then provide\ncomprehensive overview of the performance of these spatial CV methods across a\nwide array of parameterizations, providing the first comparative performance\nevaluation for many of these techniques. We found that spatial CV methods\nyielded overall better estimates of model performance than random assignment.\nApproaches that incorporated both $D_{\\operatorname{out}}$ of spatially conjunct\nobservations and buffers yielded the best results. Lastly, to facilitate further\nevaluation and use of spatial CV methods, we introduce the `spatialsample` R\npackage that implements each of the approaches evaluated here, and avoids many\nof the pitfalls encountered by prior implementations.\n\n# `spatialsample` and the tidymodels Framework\n\nThe tidymodels framework is a set of open-source packages for the R programming\nlanguage that provide a consistent interface for common modeling tasks across a\nvariety of model families and objectives following the same fundamental design\nprinciples as the tidyverse [@R; @tmwr; @wickham2019]. These packages help users\nfollow best practices while fitting and evaluating models, with functions and\noutputs that integrate well with the other packages in the tidymodels and\ntidyverse ecosystems as well as with the rest of the R modeling ecosystem.\nHistorically, data splitting and CV in the tidymodels framework has been handled\nby the `rsample` package [@rsample], with additional components of the\ntidymodels ecosystem providing functionality for hyperparameter tuning and model\nevaluation [@tune; @dials]. However, `rsample` primarily focuses on randomized\nCV approaches, and as such it has historically been difficult to implement\nspatial CV approaches within the tidymodels framework.\n\nA new tidymodels package, `spatialsample`, addresses this gap by providing a\nsuite of functions to implement the most popular spatial CV approaches. The\nresamples created by `spatialsample` rely on the same infrastructure as those\ncreated by `rsample`, and as such can make use of the same tidymodels packages\nfor hyperparameter tuning and model evaluation. As an implementation of spatial\nCV methods, `spatialsample` improves on alternate implementations in R by\nrelying on the `sf` and `s2` packages for calculating distance in geographic\ncoordinate reference systems, improving the accuracy of distance calculations\nand CV fold assignments when working with geographic coordinates [@sf; @s2]. The\n`spatialsample` package is also able to appropriately handle data with\ncoordinate reference systems that use linear units other than meters and to\nprocess user-provided parameters using any units understood by the `units`\npackage [@units]. The `spatialsample` package also allows users to perform\nspatial CV procedures while modeling data with polygon geometries, such as those\nprovided by the US Census Bureau. Distances between polygons are calculated\nbetween polygon edges, rather than centroids or other internal points, to ensure\nthat adjacent polygons are handled appropriately by CV functions. Finally,\n`spatialsample` provides a standard interface to apply inclusion radii and\nexclusion buffers to all CV methodologies (@sec-overview), providing a high\ndegree of flexibility for specifying spatial CV patterns.\n\n# Resampling Methods {#sec-overview}\n\nThis paper evaluates a number of the most popular spatial CV approaches, based\nupon their prevalence across the literature and software implementations. We\nfocus on CV methods which automatically split data either randomly or spatially\nbut did not address any assessment methods that divide data based upon\npre-specified, user-defined boundaries or predictor space. As such, we use\n\"distance\" and related terms to refer to spatial distances between observations,\nunless we specifically refer to \"distance in predictor space\".\n\n## Resubstitution\n\nEvaluating a model's performance when predicting the same data used to fit the\nmodel yields what is commonly known as the \"apparent performance\" or\n\"resubstitution performance\" of the model [@Kuhn2013]. This procedure typically\nproduces an overly-optimistic estimate of model performance, and as such is a\npoor method for estimating how well a model will generalize to new data\n[@efron1986; @gong1986; @efron1983]. We include an assessment of resubstitution\nerror in this study for comparison, but do not recommend it as an evaluation\nprocedure in practice.\n\n## Randomized V-fold CV\n\nPerhaps the most common approach to CV is V-fold CV, also known as k-fold CV. In\nthis method, each observation is randomly assigned to one of $v$ folds. Models\nare then fit to each unique combination of $v - 1$ folds and evaluated against\nthe remaining fold, with performance metrics estimated by averaging across the\n$v$ iterations [@Stone1974]. V-fold CV is generally believed to be the least\nbiased of the dominant randomized CV approaches [@kuhn2019], though research has\nsuggested it overestimates model performance [@varma2006; @Bates2021]. This\noptimistic bias is even more notable when training models using data with\ninternal dependency structures, such as spatial structure [@Roberts2017].\n\nFor this study, V-fold CV was performed using the `vfold_cv()` function in\n`rsample`.\n\n## Blocked CV\n\nOne of the most straightforward forms of spatial CV is to divide the study area\ninto a set of polygons using a regular grid, with all observations inside a\ngiven polygon assigned to $D_{\\operatorname{out}}$ as a group [@valavi2018;\n@brenning2012; @wenger2012]. This technique is known as \"spatial blocking\"\n[@Roberts2017] or \"spatial tiling\" [@brenning2012]. Frequently, each block is\nused as an independent $D_{\\operatorname{out}}$ [leave-one-block-out CV,\n@wenger2012], though many implementations allow users to use fewer\n$D_{\\operatorname{out}}$, combining multiple blocks into sets either at random\nor via a systematic assignment approach [@valavi2018]. Spatial blocking attempts\nto address the limitations of randomized V-fold CV by introducing distance\nbetween $D_{\\operatorname{in}}$ and $D_{\\operatorname{out}}$, though the\ndistance from observations on the perimeter of a block to\n$D_{\\operatorname{in}}$ will be much less than that from observations near the\ncenter of the block [@osullivan2010]. This disparity can be addressed through\nthe use of an exclusion buffer around $D_{\\operatorname{out}}$, wherein points\nwithin a certain distance of $D_{\\operatorname{out}}$ (depending on the\nimplementation, calculated alternatively as distance from the polygon defining\neach block, the convex hull of points in $D_{\\operatorname{out}}$, or from each\npoint in $D_{\\operatorname{out}}$ independently) are excluded from both\n$D_{\\operatorname{out}}$ and $D_{\\operatorname{in}}$ [@valavi2018].\n\nA challenge with spatial blocking is that dividing the study area using a\nstandard grid often results in observations in unrelated areas being grouped\ntogether in a single fold, as regular gridlines likely will not align with\nrelevant environmental features (for instance, blocks may span significant\naltitudinal gradients, or reach across a river to combine disjunct populations).\nAlthough this can be mitigated through careful parameterization of the grid, it\nis difficult to create meaningful $D_{\\operatorname{out}}$ while still enforcing\nthe required spatial separation between $D_{\\operatorname{in}}$ and\n$D_{\\operatorname{out}}$.\n\nFor this study, spatial blocking was performed using the `spatial_block_cv()`\nfunction in `spatialsample`. Each block was treated as a unique fold\n(leave-one-block-out CV).\n\n## Clustered CV\n\nAnother form of spatial CV involves grouping observations into a pre-specified\nnumber of clusters based on their spatial arrangement, and then treating each\ncluster as a $D_{\\operatorname{out}}$ in V-fold CV [@brenning2012;\n@walvoort2010]. This approach allows for a great degree of flexibility, as\nalternative distance calculations and clustering algorithms may produce\nsubstantially different clusters. Similarly to spatially-blocked CV this\napproach may produce unbalanced $D_{\\operatorname{out}}$ and folds that combine\nunrelated areas, though in practice most clustering algorithms typically produce\nmore sensible fold boundaries than spatial blocking. Clustering is also similar\nto spatial blocking in that observations closer to the center of a cluster will\nbe much further spatially separated from $D_{\\operatorname{in}}$ than those near\nthe perimeter, although clustering algorithms typically produce more circular\ntiles than blocking methods, and as such generally have fewer points along their\nperimeter overall. As with spatial blocking, exclusion buffers can be used to\nensure a minimum distance between $D_{\\operatorname{in}}$ and\n$D_{\\operatorname{out}}$.\n\nA notable difference between spatial clustering and spatial blocking is that,\ndepending upon the algorithm used to assign data to clusters, cluster boundaries\nmay be non-deterministic. This stochasticity means that repeated CV may be more\nmeaningful with clustered CV than with spatial blocking, but also makes it\ndifficult to ensure that cluster boundaries align with meaningful boundaries.\n\nFor this study, spatial clustering was performed using the\n`spatial_clustering_cv()` function in `spatialsample`. Each cluster was treated\nas a unique fold (leave-one-cluster-out CV).\n\n## Buffered Leave-One-Observation-Out CV (BLO3 CV)\n\nAn alternative approach to spatial CV involves performing leave-one-out CV, a\nform of V-fold cross validation where $v$ is set to the number of observations\nsuch that each observation forms a separate $D_{\\operatorname{out}}$, with all\npoints within a buffer distance of $D_{\\operatorname{out}}$ omitted from\n$D_{\\operatorname{in}}$ [@telford2009; @pohjankukka2017]. As the other methods\ninvestigated here are all examples of leave-one-group-out CV, with groups\ndefined by spatial positions, we refer to this procedure as buffered\nleave-one-observation-out CV (BLO3 CV). This approach may be more robust to\ndifferent parameterizations than spatial clustering or blocking, as the contents\nof a given $D_{\\operatorname{out}}$ are not as dependent upon the precise\nlocations of blocking polygons or cluster boundaries. Many studies have\nrecommended buffered leave-one-out cross validation for models fit using spatial\ndata, with the size of the exclusion buffer variously determined by variogram\nranges for model predictors, model outcomes, or model residuals [@lerest2014;\n@Roberts2017; @telford2009; @valavi2018; @karasiak2021].\n\nFor this study, BLO3 CV was performed using the `spatial_buffer_vfold_cv()`\nfunction in `spatialsample`.\n\n## Leave-One-Disc-Out CV (LODO CV)\n\nThe final spatial CV method investigated here is leave-one-disc-out CV,\nfollowing @brenning2012. This method extends BLO3 by adding all points within a\ncertain radius of each observation to $D_{\\operatorname{out}}$, increasing the\nsize of the final $D_{\\operatorname{out}}$. Data points falling within the\nexclusion buffer of any observation in $D_{\\operatorname{out}}$, including those\nadded by the inclusion radius, is then removed from $D_{\\operatorname{out}}$\n(@fig-maps). Similarly to blocked and clustered CV, LODO CV evaluates models\nagainst multiple $D_{\\operatorname{out}}$ of spatially conjunct observations.\nSimilar to BLO3 CV, LODO CV approach may be more robust to different parameter\nvalues than methods assigning $D_{\\operatorname{out}}$ based upon blocking\npolygons or cluster boundaries. Unlike any of the other approaches investigated,\nobservations may appear in multiple $D_{\\operatorname{out}}$, with observations\nin more intensively sampled regions being selected more often.\n\nIn this study, spatial leave-one-disc-out CV was performed using the\n`spatial_buffer_vfold_cv()` function in `spatialsample`. An important feature of\nthis implementation of leave-disc-out CV is that the exclusion buffer is\ncalculated separately for each point in $D_{\\operatorname{out}}$. Where other\nimplementations remove all observations within the buffer distance of the\ninclusion radius to create a uniform \"doughnut\" shaped buffer, `spatialsample`\nonly removes observations that are within the buffer distance of data in\n$D_{\\operatorname{out}}$, potentially retaining more data in\n$D_{\\operatorname{in}}$ by creating an irregular buffer polygon.\n\n# Methods {#sec-methods}\n\n## Landscape Simulation {#sec-simulation}\n\nTo compare the validation techniques described above, we extended the simulation\napproach used by Roberts et al. [-@Roberts2017, Box 1]. We simulated 100\nlandscapes, representing independent realizations of the same data-generating\nprocess, generating a set of 13 variables calculated using the same stochastic\nformulations across a regularly spaced 50 x 50 cell grid, for a total of 2,500\ncells per landscape (@tbl-simulations). Simulated predictors included eight\nrandom Gaussian fields, generated using the `RandomFields` R package\n[@RandomFields], which uses stationary isotropic covariance models to generate\nspatially structured variables. Five additional variables were calculated as\ncombinations of the randomly generated variables to imitate interactions between\nenvironmental variables. This simulation approach was originally designed to\nresemble the environmental data that might be used to model species abundance\nand distribution; further interpretation of what each predictor represents is\nprovided in Appendix 2 of @Roberts2017.\n\n\n\n\n::: {#tbl-simulations .cell tbl-cap='Simulated predictors generated for each independent landscape, created following Roberts et al. 2017. Predictors are indicated as being used for $y$ if they were included either in Equation 1 or used to calculate variables included in Equation 1. Predictors are indicated as being used in models if they were used as predictors in random forest models.'}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Name </th>\n   <th style=\"text-align:left;\"> Variable Definition </th>\n   <th style=\"text-align:left;\"> Used for $y$? </th>\n   <th style=\"text-align:left;\"> Used in model? </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> X1 </td>\n   <td style=\"text-align:left;width: 20em; \"> Random Gaussian field with exponential covariance (variance = 0.1, scale = 0.1) </td>\n   <td style=\"text-align:left;\"> Yes </td>\n   <td style=\"text-align:left;width: 5em; \"> No </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> X2 </td>\n   <td style=\"text-align:left;width: 20em; \"> Random Gaussian field with exponential covariance (variance = 0.3, scale = 0.1) </td>\n   <td style=\"text-align:left;\"> No </td>\n   <td style=\"text-align:left;width: 5em; \"> Yes </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> X3 </td>\n   <td style=\"text-align:left;width: 20em; \"> Random Gaussian field with Gaussian covariance (variance = 0.1, scale = 0.3) </td>\n   <td style=\"text-align:left;\"> No </td>\n   <td style=\"text-align:left;width: 5em; \"> Yes </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> X4 </td>\n   <td style=\"text-align:left;width: 20em; \"> If the ratio (X2 / X3) is above the 95th percentile of all values, 0; else 1. </td>\n   <td style=\"text-align:left;\"> Yes (excluding) </td>\n   <td style=\"text-align:left;width: 5em; \"> No </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> X5 </td>\n   <td style=\"text-align:left;width: 20em; \"> X1 + X2 + X3 + (X2 $\\cdot$ X3) </td>\n   <td style=\"text-align:left;\"> Yes </td>\n   <td style=\"text-align:left;width: 5em; \"> No </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> X6 </td>\n   <td style=\"text-align:left;width: 20em; \"> Random Gaussian field with exponential covariance (variance = 0.1, scale = 0.1) </td>\n   <td style=\"text-align:left;\"> Yes </td>\n   <td style=\"text-align:left;width: 5em; \"> Yes </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> X7 </td>\n   <td style=\"text-align:left;width: 20em; \"> Random Gaussian field with exponential covariance (variance = 0.1, scale = 0.1) </td>\n   <td style=\"text-align:left;\"> No </td>\n   <td style=\"text-align:left;width: 5em; \"> Yes </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> X8 </td>\n   <td style=\"text-align:left;width: 20em; \"> Random Gaussian field with exponential covariance (variance = 0.1, scale = 0.1) </td>\n   <td style=\"text-align:left;\"> No </td>\n   <td style=\"text-align:left;width: 5em; \"> Yes </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> X9 </td>\n   <td style=\"text-align:left;width: 20em; \"> Random Gaussian field with Gaussian covariance (variance = 0.1, scale = 0.3) </td>\n   <td style=\"text-align:left;\"> No </td>\n   <td style=\"text-align:left;width: 5em; \"> Yes </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> X10 </td>\n   <td style=\"text-align:left;width: 20em; \"> Random Gaussian field with Gaussian covariance (variance = 0.1, scale = 0.3) </td>\n   <td style=\"text-align:left;\"> No </td>\n   <td style=\"text-align:left;width: 5em; \"> Yes </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> X11 </td>\n   <td style=\"text-align:left;width: 20em; \"> X2/X3 </td>\n   <td style=\"text-align:left;\"> Yes (limiting) </td>\n   <td style=\"text-align:left;width: 5em; \"> No </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> X12 </td>\n   <td style=\"text-align:left;width: 20em; \"> $1 / (\\operatorname{sqrt}(2\\cdot\\pi))\\cdot\\exp-(\\operatorname{X3}^2/4)$ </td>\n   <td style=\"text-align:left;\"> Yes </td>\n   <td style=\"text-align:left;width: 5em; \"> No </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> X13 </td>\n   <td style=\"text-align:left;width: 20em; \"> $1 / (\\operatorname{sqrt}(2\\cdot\\pi))\\cdot\\exp-(\\operatorname{X2}^2/4)$ </td>\n   <td style=\"text-align:left;\"> Yes </td>\n   <td style=\"text-align:left;width: 5em; \"> No </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n\nThese predictors were then used to generate a target variable $y$ using @eq-y.\n\n$$\ny = \n\\left\\{\n    \\begin{array}{lr}\n        \\min(y), & \\text{if } \\operatorname{X4} \\neq 0\\\\\n        \\operatorname{X11}, & \\text{if } y\\geq \\operatorname{X11}\\\\\n        \\operatorname{X1} + \\operatorname{X5} + \\operatorname{X6} + \\operatorname{X12} + \\operatorname{X13}, & \\text{otherwise }\n    \\end{array}\n\\right\\}\n$$ {#eq-y}\n\nOne instance of the spatially clustered $y$ values produced by this process is\nvisualized in @fig-maps.\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![One simulation of the dependent variable $y$ and several of the CV approaches used. A: Simulated data showing environmental clustering of a variable of interest, named \"Target\", in one simulated landscape. B: Spatial clustering CV fold assignments, as produced by `spatial_clustering_cv()`, based upon using k-means clustering to group observations spatially. C: Spatially blocked CV fold assignments, produced using `spatial_block_cv()` to group observations spatially. D: A single fold of LODO CV, showing $D_{\\operatorname{in}}$ and $D_{\\operatorname{out}}$ as well as the exclusion buffer, performed using `spatial_buffer_vfold_cv()` with a radius and buffer covering 10% of the mapped area. Points within the inclusion radius of the randomly selected observation are included in $D_{\\operatorname{out}}$, while points within the exclusion buffer of $D_{\\operatorname{out}}$ are not included in either set. For BLO3 and LODO CV, this procedure is repeated for each grid cell.](paper_files/figure-html/fig-maps-1.png){#fig-maps width=8976.3779527559}\n:::\n:::\n\n\n\n\nModels were then fit using variables X2, X3, and X6 - X10. Of these seven\nvariables, three were involved in calculating the target variable $y$ (X2 and\nX3, as components of X4 and X5; and X6, used directly) and therefore provide\nuseful information for models, while the remaining four (X7 - X10) were included\nto allow overfitting.\n\n## Resampling Methodology {#sec-resampling}\n\nWe divided each simulated landscape into folds using each of the data splitting\napproaches (@sec-overview) across a wide range of parameter sets\n(@tbl-whichparams; @tbl-paramdefs) in order to evaluate the usefulness of\nspatial CV approaches. Spatial blocking, spatial clustering, and\nleave-one-disc-out used a \"leave-one-group-out\" approach, where each\n$D_{\\operatorname{out}}$ was made up of a single block or cluster of\nobservations, with all other data (excluding any within the exclusion buffer)\nused as $D_{\\operatorname{in}}$. BLO3 used a leave-one-observation-out approach.\nWe additionally evaluated spatial blocking with fewer $D_{\\operatorname{out}}$\nthan blocks, resulting in multiple blocks being used in each\n$D_{\\operatorname{out}}$. Each simulated landscape was resampled independently,\nmeaning that stochastic methods (such as V-fold CV and spatial clustering)\nproduced different CV folds across each simulation. All resampling used\nfunctions implemented in the `rsample` and `spatialsample` packages [@rsample;\n@spatialsample]. Examples of spatial clustering CV, spatially blocked CV, and\nleave-one-disc-out CV are visualized in @fig-maps.\n\n\n\n\n::: {#tbl-whichparams .cell tbl-cap='Parameters applied to each CV method assessed, and the number of iterations performed. 100 iterations were performed per unique combination of parameters.'}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> CV Method </th>\n   <th style=\"text-align:left;\"> Resampling function </th>\n   <th style=\"text-align:left;\"> Parameters </th>\n   <th style=\"text-align:right;\"> # of iterations </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Resubstitution </td>\n   <td style=\"text-align:left;font-family: monospace;\">  </td>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:right;\"> 100 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> V-fold </td>\n   <td style=\"text-align:left;font-family: monospace;\"> vfold_cv() </td>\n   <td style=\"text-align:left;\"> V </td>\n   <td style=\"text-align:right;\"> 400 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Blocked </td>\n   <td style=\"text-align:left;font-family: monospace;\"> spatial_block_cv() </td>\n   <td style=\"text-align:left;\"> Block size, Buffer </td>\n   <td style=\"text-align:right;\"> 8800 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Clustered </td>\n   <td style=\"text-align:left;font-family: monospace;\"> spatial_clustering_cv() </td>\n   <td style=\"text-align:left;\"> V, Buffer, Cluster function </td>\n   <td style=\"text-align:right;\"> 8800 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> BLO3 </td>\n   <td style=\"text-align:left;font-family: monospace;\"> spatial_buffer_vfold_cv() </td>\n   <td style=\"text-align:left;\"> Buffer </td>\n   <td style=\"text-align:right;\"> 1700 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> LODO </td>\n   <td style=\"text-align:left;font-family: monospace;\"> spatial_buffer_vfold_cv() </td>\n   <td style=\"text-align:left;\"> Buffer, Radius </td>\n   <td style=\"text-align:right;\"> 11100 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n::: {#tbl-paramdefs .cell tbl-cap='Definitions of parameters applied to CV methods.'}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Parameter </th>\n   <th style=\"text-align:left;\"> Values </th>\n   <th style=\"text-align:left;\"> Definition </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> V </td>\n   <td style=\"text-align:left;width: 15em; \"> 2, 5, 10, 20; 2, 4, 9, 16, 25, 36, 64, 100 </td>\n   <td style=\"text-align:left;width: 20em; \"> The number of folds to assign data into. Each fold was used as $D_{\\operatorname{out}}$ precisely once. The first set of values were used for spatial clustering, while the second was used for spatial blocking. For spatial clustering, this controls the number of clusters. </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Cluster function </td>\n   <td style=\"text-align:left;width: 15em; \"> K-means, Hierarchical </td>\n   <td style=\"text-align:left;width: 20em; \"> The algorithm used to cluster observations into folds. </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Block size </td>\n   <td style=\"text-align:left;width: 15em; \"> 1/100, 1/64, 1/36, 1/25, 1/16, 1/9, 1/4, 1/2 </td>\n   <td style=\"text-align:left;width: 20em; \"> The proportion of the grid each block should occupy (such that 1/2 creates two blocks, each occupying half the grid). </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Blocking method </td>\n   <td style=\"text-align:left;width: 15em; \"> Random, Systematic (continuous), Systematic (snake) </td>\n   <td style=\"text-align:left;width: 20em; \"> For spatial blocking, the method for assigning blocks to folds: randomly ('random'), in a 'scanline' moving left to right across each row of the grid ('systematic (continuous)'), or moving back and forth across the rows of the grid ('systematic (snake)'). </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Buffer </td>\n   <td style=\"text-align:left;width: 15em; \"> 0.00, 0.03, 0.06, 0.09, 0.12, 0.15, 0.18, 0.21, 0.24, 0.27, 0.30, 0.33, 0.36, 0.39, 0.42, 0.45, 0.48 </td>\n   <td style=\"text-align:left;width: 20em; \"> The size of the exclusion buffer to apply around $D_{\\operatorname{out}}$, expressed as a proportion of the side length of the grid. Observations within this distance of any point in $D_{\\operatorname{out}}$ are included in neither $D_{\\operatorname{in}}$ nor $D_{\\operatorname{out}}$. Buffer distances above 0.3 were only used for BLO3 CV, as increased buffer distances around larger $D_{\\operatorname{out}}$ may produce empty $D_{\\operatorname{in}}$. </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Radius </td>\n   <td style=\"text-align:left;width: 15em; \"> 0.00, 0.03, 0.06, 0.09, 0.12, 0.15, 0.18, 0.21, 0.24, 0.27, 0.30 </td>\n   <td style=\"text-align:left;width: 20em; \"> The size of the inclusion radius to apply around $D_{\\operatorname{out}}$, expressed as a proportion of the side length of the grid. Observations within this distance of any point in $D_{\\operatorname{out}}$ are moved from $D_{\\operatorname{in}}$ into $D_{\\operatorname{out}}$. </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n\n## Model Fitting and Evaluation {#sec-models}\n\nFor each iteration, we modeled the target variable $y$ using random forests as\nimplemented in the `ranger` R package [@Breiman2001; @ranger], fit using\nvariables X2, X3, and X6 - X10. Random forests generally provide high predictive\naccuracy even without hyperparameter tuning [@Probst2018], and as such all\nrandom forests were fit using the default hyperparameter settings of the\n`ranger` package, namely 500 decision trees, a minimum of 5 observations per\nleaf node, and two variables to split on per node.\n\nModel accuracy was measured using root-mean-squared error (RMSE, @eq-rmse). To\nfind the \"ideal\" error rate that we would expect CV approaches to estimate, we\nfit 100 separate random forest models, each trained using all values within one\nof the 100 simulated landscapes. We then calculated the RMSE for each of these\nmodels when used to predict each of the 99 other landscapes. As each landscape\nis an independent realization of the same data-generation process, the\nrelationships between predictors and $y$ is identical across landscapes,\nalthough the spatial relationships between $y$ and variables not used to\ngenerate $y$ are likely different across iterations. As such, RMSE values from a\nmodel trained on one landscape and used to predict the others represent the\nability of the model to predict $y$ based upon the predictors and without\nrelying upon spatial structure. These RMSE estimates therefore represent the\n\"true\" range of RMSE values when using these models for spatial extrapolation to\nareas with the same relationship between predictors and the target feature, but\nwithout any spatial correlation to the training data itself. We defined the\nsuccess of model evaluation methods as the proportion of iterations which\nreturned RMSE estimates between the 5th and 9th percentile RMSEs of this \"ideal\"\nestimation procedure.\n\nTo find the error rate of the resubstitution approach, we fit 100 random\nforests, one to each landscape, and then calculated the RMSE for each model when\nused to predict its own training data. To find the error of each CV approach, we\nfirst used each CV approach to separate each landscape into $n$ folds\n(@sec-resampling). We then fit models to each combination of $n - 1$ of these\nfolds, and calculated RMSE when using the model to predict the remaining\n$D_{\\operatorname{out}}$ (@eq-rmse).\n\n$$\n\\operatorname{RMSE} = \\sqrt{(\\frac{1}{n})\\sum_{i=1}^{n}(y_{i} - \\hat{y_{i}})^{2}}\n$$ {#eq-rmse}\n\nWe then calculated the variance of the RMSE estimates of each method across the\n100 simulated landscapes, as well as the proportion of runs for each method\nwhich fell between the 5th and 95th percentiles of the \"true\" RMSE range.\n\nBased upon prior research, we expected the optimal spacing between\n$D_{\\operatorname{in}}$ and $D_{\\operatorname{out}}$ to be related to the range\nof spatial dependence either in the outcome variable or in model residuals\n[@lerest2014; @Roberts2017; @telford2009]. As such, we quantified the range of\nspatial autocorrelation in both the target variable $y$ and in resubstitution\nresiduals from random forest models using the automated variogram fitting\napproach implemented in the `automap` R package [@automap].\n\n# Results and Discussion {#sec-results}\n\n## Spatial CV Improves Model Performance Estimates\n\nSpatial cross-validation methods consistently produced more accurate estimates\nof model performance than non-spatial methods, which were optimistically biased\n(producing too-low estimates of RMSE) (@tbl-overall; @fig-comparisons). CV\nproduced the best estimates when $D_{\\operatorname{out}}$ of spatially conjunct\nobservations were combined with exclusion buffers (@tbl-winners). Spatially\nclustered CV and LODO, both of which enforce $D_{\\operatorname{out}}$ of\nspatially conjunct observations, were among the most consistently effective CV\nmethods (@fig-comparisons). Removing too much data from $D_{\\operatorname{in}}$,\nsuch as by clustering with only two folds or blocking with only two blocks\nresulted in pessimistic over-estimates of RMSE (@fig-rmse-delta).\n\n\n\n\n::: {#tbl-overall .cell tbl-cap='Mean RMSE estimates across all evaluated parameterizations of cross-validation strategies. Numbers in parentheses represent standard deviations. \"% within target RMSE range\" refers to the percentage of iterations which had RMSE estimates between the 5th and 95th percentile estimates from the true RMSE estimation procedure.'}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Method </th>\n   <th style=\"text-align:left;\"> RMSE </th>\n   <th style=\"text-align:left;\"> % within target RMSE range </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Ideal RMSE </td>\n   <td style=\"text-align:left;\"> 0.715 (0.042) </td>\n   <td style=\"text-align:left;width: 7em; \"> 90.00% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Clustered </td>\n   <td style=\"text-align:left;\"> 0.743 (0.161) </td>\n   <td style=\"text-align:left;width: 7em; \"> 36.97% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> LODO </td>\n   <td style=\"text-align:left;\"> 0.641 (0.135) </td>\n   <td style=\"text-align:left;width: 7em; \"> 31.70% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Blocked </td>\n   <td style=\"text-align:left;\"> 0.664 (0.159) </td>\n   <td style=\"text-align:left;width: 7em; \"> 27.90% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> V-fold </td>\n   <td style=\"text-align:left;\"> 0.440 (0.076) </td>\n   <td style=\"text-align:left;width: 7em; \"> 2.00% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> BLO3CV </td>\n   <td style=\"text-align:left;\"> 0.429 (0.098) </td>\n   <td style=\"text-align:left;width: 7em; \"> 1.29% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Resubstitution </td>\n   <td style=\"text-align:left;\"> 0.189 (0.032) </td>\n   <td style=\"text-align:left;width: 7em; \"> 0.00% </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![RMSE distributions of each spatial CV method evaluated. The x axis represents the distance of an RMSE from the mean \"true\" RMSE values, such that negative values underpredict the true RMSE and positive values overpredict it.  Distributions are scaled relative to the number of iterations run, such that a unit area represents the same number of iterations across each method, but not necessarily the same proportion of all iterations evaluated. The green rectangle represents the 90% interval of true RMSE values used as the \"target\" RMSE range.](paper_files/figure-html/fig-comparisons-1.png){#fig-comparisons width=8976.3779527559}\n:::\n:::\n\n::: {#tbl-winners .cell tbl-cap='Mean RMSE estimates across 100 iterations of various cross-validation strategies (bold headers) for the parameterizations producing the most accurate model performance estimates. Numbers in parentheses represent standard deviations. \"% within target RMSE range\" refers to the percentage of iterations which had RMSE estimates between the 5th and 95th percentile estimates from the true RMSE estimation procedure.'}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> V </th>\n   <th style=\"text-align:left;\"> Cell size </th>\n   <th style=\"text-align:left;\"> Cluster function </th>\n   <th style=\"text-align:right;\"> Exclusion buffer </th>\n   <th style=\"text-align:right;\"> Inclusion radius </th>\n   <th style=\"text-align:left;\"> RMSE </th>\n   <th style=\"text-align:left;\"> % within target RMSE range </th>\n  </tr>\n </thead>\n<tbody>\n  <tr grouplength=\"1\"><td colspan=\"7\" style=\"border-bottom: 1px solid;\"><strong>Ideal RMSE</strong></td></tr>\n<tr>\n   <td style=\"text-align:right;padding-left: 2em;\" indentlevel=\"1\">  </td>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:right;\">  </td>\n   <td style=\"text-align:right;\">  </td>\n   <td style=\"text-align:left;\"> 0.715 (0.042) </td>\n   <td style=\"text-align:left;width: 7em; \"> 90.00% </td>\n  </tr>\n  <tr grouplength=\"3\"><td colspan=\"7\" style=\"border-bottom: 1px solid;\"><strong>Clustered</strong></td></tr>\n<tr>\n   <td style=\"text-align:right;padding-left: 2em;\" indentlevel=\"1\"> 10 </td>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:left;\"> kmeans </td>\n   <td style=\"text-align:right;\"> 0.15 </td>\n   <td style=\"text-align:right;\">  </td>\n   <td style=\"text-align:left;\"> 0.694 (0.087) </td>\n   <td style=\"text-align:left;width: 7em; \"> 60.00% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;padding-left: 2em;\" indentlevel=\"1\"> 5 </td>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:left;\"> kmeans </td>\n   <td style=\"text-align:right;\"> 0.09 </td>\n   <td style=\"text-align:right;\">  </td>\n   <td style=\"text-align:left;\"> 0.723 (0.099) </td>\n   <td style=\"text-align:left;width: 7em; \"> 59.00% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;padding-left: 2em;\" indentlevel=\"1\"> 10 </td>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:left;\"> kmeans </td>\n   <td style=\"text-align:right;\"> 0.18 </td>\n   <td style=\"text-align:right;\">  </td>\n   <td style=\"text-align:left;\"> 0.712 (0.094) </td>\n   <td style=\"text-align:left;width: 7em; \"> 59.00% </td>\n  </tr>\n  <tr grouplength=\"3\"><td colspan=\"7\" style=\"border-bottom: 1px solid;\"><strong>LODO</strong></td></tr>\n<tr>\n   <td style=\"text-align:right;padding-left: 2em;\" indentlevel=\"1\">  </td>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:right;\"> 0.18 </td>\n   <td style=\"text-align:right;\"> 0.21 </td>\n   <td style=\"text-align:left;\"> 0.718 (0.095) </td>\n   <td style=\"text-align:left;width: 7em; \"> 60.00% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;padding-left: 2em;\" indentlevel=\"1\">  </td>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:right;\"> 0.12 </td>\n   <td style=\"text-align:right;\"> 0.24 </td>\n   <td style=\"text-align:left;\"> 0.703 (0.093) </td>\n   <td style=\"text-align:left;width: 7em; \"> 59.00% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;padding-left: 2em;\" indentlevel=\"1\">  </td>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:right;\"> 0.12 </td>\n   <td style=\"text-align:right;\"> 0.27 </td>\n   <td style=\"text-align:left;\"> 0.725 (0.098) </td>\n   <td style=\"text-align:left;width: 7em; \"> 59.00% </td>\n  </tr>\n  <tr grouplength=\"3\"><td colspan=\"7\" style=\"border-bottom: 1px solid;\"><strong>Blocked</strong></td></tr>\n<tr>\n   <td style=\"text-align:right;padding-left: 2em;\" indentlevel=\"1\">  </td>\n   <td style=\"text-align:left;\"> 1/9 </td>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:right;\"> 0.24 </td>\n   <td style=\"text-align:right;\">  </td>\n   <td style=\"text-align:left;\"> 0.738 (0.099) </td>\n   <td style=\"text-align:left;width: 7em; \"> 61.00% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;padding-left: 2em;\" indentlevel=\"1\">  </td>\n   <td style=\"text-align:left;\"> 1/9 </td>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:right;\"> 0.21 </td>\n   <td style=\"text-align:right;\">  </td>\n   <td style=\"text-align:left;\"> 0.732 (0.100) </td>\n   <td style=\"text-align:left;width: 7em; \"> 60.00% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;padding-left: 2em;\" indentlevel=\"1\">  </td>\n   <td style=\"text-align:left;\"> 1/25 </td>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:right;\"> 0.27 </td>\n   <td style=\"text-align:right;\">  </td>\n   <td style=\"text-align:left;\"> 0.688 (0.084) </td>\n   <td style=\"text-align:left;width: 7em; \"> 58.00% </td>\n  </tr>\n  <tr grouplength=\"3\"><td colspan=\"7\" style=\"border-bottom: 1px solid;\"><strong>V-fold</strong></td></tr>\n<tr>\n   <td style=\"text-align:right;padding-left: 2em;\" indentlevel=\"1\"> 2 </td>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:right;\">  </td>\n   <td style=\"text-align:right;\">  </td>\n   <td style=\"text-align:left;\"> 0.475 (0.079) </td>\n   <td style=\"text-align:left;width: 7em; \"> 2.00% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;padding-left: 2em;\" indentlevel=\"1\"> 5 </td>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:right;\">  </td>\n   <td style=\"text-align:right;\">  </td>\n   <td style=\"text-align:left;\"> 0.438 (0.073) </td>\n   <td style=\"text-align:left;width: 7em; \"> 2.00% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;padding-left: 2em;\" indentlevel=\"1\"> 10 </td>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:right;\">  </td>\n   <td style=\"text-align:right;\">  </td>\n   <td style=\"text-align:left;\"> 0.428 (0.071) </td>\n   <td style=\"text-align:left;width: 7em; \"> 2.00% </td>\n  </tr>\n  <tr grouplength=\"3\"><td colspan=\"7\" style=\"border-bottom: 1px solid;\"><strong>BLO3</strong></td></tr>\n<tr>\n   <td style=\"text-align:right;padding-left: 2em;\" indentlevel=\"1\">  </td>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:right;\"> 0.48 </td>\n   <td style=\"text-align:right;\">  </td>\n   <td style=\"text-align:left;\"> 0.524 (0.070) </td>\n   <td style=\"text-align:left;width: 7em; \"> 7.00% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;padding-left: 2em;\" indentlevel=\"1\">  </td>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:right;\"> 0.45 </td>\n   <td style=\"text-align:right;\">  </td>\n   <td style=\"text-align:left;\"> 0.516 (0.069) </td>\n   <td style=\"text-align:left;width: 7em; \"> 4.00% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;padding-left: 2em;\" indentlevel=\"1\">  </td>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:right;\"> 0.42 </td>\n   <td style=\"text-align:right;\">  </td>\n   <td style=\"text-align:left;\"> 0.508 (0.067) </td>\n   <td style=\"text-align:left;width: 7em; \"> 3.00% </td>\n  </tr>\n  <tr grouplength=\"1\"><td colspan=\"7\" style=\"border-bottom: 1px solid;\"><strong>Resubstitution</strong></td></tr>\n<tr>\n   <td style=\"text-align:right;padding-left: 2em;\" indentlevel=\"1\">  </td>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:right;\">  </td>\n   <td style=\"text-align:right;\">  </td>\n   <td style=\"text-align:left;\"> 0.189 (0.032) </td>\n   <td style=\"text-align:left;width: 7em; \"> 0.00% </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![Spatial CV RMSE estimates under various parameterizations. Colors represent the distance of an RMSE from the mean \"true\" RMSE values, such that negative values underpredict the true RMSE and positive values overpredict it. A: Spatial clustering RMSE estimates using different numbers of clusters (\"V\") and different sizes of exclusion buffers (\"Buffer\"). B: Spatial blocking RMSE estimates using different sizes of blocks (\"Cell Size\"; a cell size of \"1/2\" implies two blocks, each containing half the study area) and different sizes of exclusion buffers (\"Buffer\"). C: LODO RMSE estimates using different sizes of inclusion radii (\"Radius\") and different sizes of exclusion buffers (\"Buffer\"). D: BLO3 RMSE estimates using different sizes of exclusion buffers (\"Buffer\").](paper_files/figure-html/fig-rmse-delta-1.png){#fig-rmse-delta width=8976.3779527559}\n:::\n:::\n\n\n\n\nThe best parameter sets for CV methods consistently separated the center of\n$D_{\\operatorname{out}}$ from $D_{\\operatorname{in}}$ by 25%-41% of the grid\nlength (Clustering 25%-29%; LODO 36%-39%; Blocked 32%-41%; BLO3 24%-30%). Given\nthat the target variable had a mean autocorrelation range of\n24.61% of the grid\nlength (@fig-ranges), this suggests that spatial cross-validation approaches\nproduce the best estimates of model performance when $D_{\\operatorname{out}}$ is\nsufficiently separated from $D_{\\operatorname{in}}$ such that there is no\nspatial dependency in the outcome variable between the two sets.\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Autocorrelation range distributions for the outcome variable and resubstitution model residuals, used throughout the literature to identify target distances to separate $D_{\\operatorname{out}}$ and $D_{\\operatorname{in}}$. Units are as a proportion of the side length of the grid. Ranges were determined via empirical variogram. Each point represents one iteration of the simulation process, while \"clouds\" represent the matching probability density function. Black points represent the median autocorrelation range, while the black bar represents the interquartile range and 95% interval.](paper_files/figure-html/fig-ranges-1.png){#fig-ranges width=8976.3779527559}\n:::\n:::\n\n\n\n\nClustering appeared to be the spatial CV method most robust to different\nparameterizations (@fig-comparisons; @fig-rmse-delta), with the highest\nproportion of all iterations within the target RMSE range (Clustered\n36.97%\nof all iterations; LODO\n31.70%;\nBlocked\n27.90%;\nBLO3\n)\n(@fig-rmse-prop). This may, however, simply reflect the relatively narrow range\nof parameters evaluated with clustering, as both blocking and LODO had a wide\nrange of parameters which returned estimates within the target range at least\nhalf the time (@fig-rmse-prop). While BLO3 exhibited increasing RMSE with\nincreasing buffer radii (@fig-rmse-delta), as frequently reported in the\nliterature, we found it only rarely produced RMSE estimates within the target\nrange (@fig-rmse-prop).\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Percentage of RMSE estimates within the target range (the 90% interval of true RMSE values) from spatial CV methods under various parameterizations. A: Spatial clustering using different numbers of clusters (\"V\") and different sizes of exclusion buffers (\"Buffer\"). B: Spatial blocking using different sizes of blocks (\"Cell Size\"; a cell size of \"1/2\" implies two blocks, each containing half the study area) and different sizes of exclusion buffers (\"Buffer\"). C: LODO using different sizes of inclusion radii (\"Radius\") and different sizes of exclusion buffers (\"Buffer\"). D: BLO3 using different sizes of exclusion buffers (\"Buffer\").](paper_files/figure-html/fig-rmse-prop-1.png){#fig-rmse-prop width=8976.3779527559}\n:::\n:::\n\n\n\n\nRMSE estimates from spatial blocking were inversely related to the number of\n$D_{\\operatorname{out}}$ used, likely due to the distance between\n$D_{\\operatorname{out}}$ and $D_{\\operatorname{in}}$ increasing if adjacent\nblocks were assigned to the same $D_{\\operatorname{out}}$ (@fig-blocks-with-v).\nRMSE estimates generally increased gradually as the number of\n$D_{\\operatorname{out}}$ decreased, though notable increases were observed when\nblocks were assigned via the continuous systematic method and the number of\ncells in each grid row were evenly divisible by the number of\n$D_{\\operatorname{out}}$ (e.g., when 1/16th cell sizes produced by a 4-by-4 grid\nwere divided into 4 folds). In these situations, each column of the grid will be\nentirely assigned to the same $D_{\\operatorname{out}}$, somewhat resembling the\nCV method of @wenger2012, producing $D_{\\operatorname{out}}$ which have no\nneighboring $D_{\\operatorname{in}}$ observations in the *y* direction and\ntherefore a greater average distance between $D_{\\operatorname{out}}$ and\n$D_{\\operatorname{in}}$. Overall, these results suggest that using fewer\n$D_{\\operatorname{out}}$ than blocks may be appropriate when the range of\nautocorrelation in the outcome variable is relatively small and there is concern\nabout large blocks restricting predictor space [@Roberts2017]; however, with\nlonger autocorrelation ranges it is likely best to use a leave-one-block-out\napproach with fewer, larger blocks.\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![RMSE estimates from spatially blocked CV with various block sizes assigned to various numbers of $D_{\\operatorname{out}}$. A: Spatial CV RMSE estimates under various parameterizations. Colors represent the distance of an RMSE from the mean \"true\" RMSE values, such that negative values underpredict the true RMSE and positive values overpredict it. B: Percentage of RMSE estimates within the target range (the 90% interval of true RMSE values). Labels above each panel refer to the method for assigning blocks to folds: either random assignment, systematically with each row assigned from left to right (\"continuous\"), or systematically with each row assigned in a \"snaking\" pattern (first row left to right, next row right to left, then repeating).](paper_files/figure-html/fig-blocks-with-v-1.png){#fig-blocks-with-v width=8976.3779527559}\n:::\n:::\n\n\n\n\nAs such, the recommendations from this study are clear: CV-based performance\nassessments of models fit using spatial data benefit from spatial CV approaches.\nThose spatial CV approaches are most likely to return good estimates of true\nmodel accuracy if they combine $D_{\\operatorname{out}}$ of spatially conjunct\nobservations with exclusion buffers, such that the average observation is\nseparated from $D_{\\operatorname{in}}$ by enough distance that there is no\nspatial dependency in the outcome variable between $D_{\\operatorname{in}}$ and\n$D_{\\operatorname{out}}$.\n\n## Limitations\n\nThis simulation study assumed that spatial CV could take advantage of regularly\ndistributed observations, such that all locations had a similar density of\nmeasurement points. This assumption is often violated, as it is often\nimpractical to obtain a uniform sample across large areas, and as such\nobservations are often clustered in more convenient locations and relatively\nsparse in less accessible areas [@meyer2022; @martin2012]. Alternative\napproaches not investigated in this study may be more effective in these\nsituations; for instance, when the expected distance between training data and\nmodel predictions is known, @mil2022 proposes an alternative nearest neighbor\ndistance matching CV approach which may equal or improve upon buffered\nleave-one-out CV. An alternative approach put forward by @meyer2018 uses\nmeaningful, human-defined locations as groups for CV, which may produce better\nresults than the automated partitioning methods investigated in this study.\nWhile we believe our results clearly demonstrate the benefits of spatial CV for\nsampling designs resembling our simulation, we do not pretend to present the one\nCV approach to rule them all.\n\nWe additionally do not expect these results, focused upon using CV to evaluate\nthe accuracy of predictive models, will necessarily transfer to map accuracy\nassessments. @stehman2019 explained that design-based sampling approaches\nprovide unbiased assessments of map accuracy, while @wadoux2021 demonstrated\nthat spatial CV methods may be overly pessimistic when assessing map accuracy,\nand @debruin2022 suggested sampling-intensity weighted CV approaches for map\naccuracy assessments in situations where the study area has been unevenly\nsampled. However, we expect these results will be informative in the many\nsituations requiring estimates of model accuracy, particularly given that\ntraditional held-out test sets are somewhat rare in the spatial modeling\nliterature.\n\nLastly, we did not investigate any CV approaches which aim to preserve outcome\nor predictor distributions across $D_{\\operatorname{out}}$. When working with\nimbalanced outcomes, random sampling may produce $D_{\\operatorname{out}}$ with\nnotably different outcome distributions than the overall training data, which\nmay bias performance estimates. Assigning stratified samples of observations to\n$D_{\\operatorname{out}}$ can address this, but it is not obvious how to use\nstratified sampling when assigning groups of observations (such as a spatial\ncluster or block), with one outcome value per observation, to\n$D_{\\operatorname{out}}$ as a unit. The `rsample` package allows stratified CV\nwhen all observations within a given group have identical outcome values (that\nis, when groups are strictly nested within the stratification variable), but\nthis condition is rare and difficult to enforce when using unsupervised group\nassignment based on spatial location, as with all the spatial CV methods we\ninvestigated.\n\nCreating $D_{\\operatorname{out}}$ based on predictor space, rather than outcome\ndistributions, has also been proposed as a solution to spatial CV procedures\nrestricting the predictor ranges present in $D_{\\operatorname{in}}$. This is a\nparticular challenge if the predictors themselves are spatially structured, and\nmay unintentionally force models to extrapolate further in predictor space than\nwould be expected when predicting new data [@Roberts2017]. As increasing\ndistance in predictor space often correlates with increasing error [e.g.\n@thuiller2004; @sheridan2004; @meyer2021], @Roberts2017 suggest blocking\napproaches to minimize distance in predictor space between folds, although to\nthe best of our knowledge these approaches are not yet in widespread use. A\nrelated field of research suggests methods for calculating the applicability\ndomain of a model [@netzeva2005; @meyer2021], which can help to identify when\npredicting new observations will require extrapolation in predictor space, and\nwill likely produce predictions with higher than expected errors. Such methods\nare particularly well-equipped to supplement spatial CV procedures, as it\nadjusts the permissible distance in predictor space based upon the distance\nbetween $D_{\\operatorname{in}}$ and $D_{\\operatorname{out}}$.\n\n# Conclusion {#sec-conclusion}\n\nThese results reinforce that spatial CV is essential for evaluating the\nperformance of predictive models fit to data with internal spatial structure,\nparticularly in situations where design-based map accuracy assessments are not\npractical or germane. Techniques that apply exclusion buffers around assessment\nsets of spatially conjunct observations, such as spatial clustering and LODO,\nare likely to produce the best estimates of model performance. The most accurate\nestimates of model performance are produced when the assessment and analysis\ndata are sufficiently separated so that there is no spatial dependence in the\noutcome variable between the two sets.\n\n# Acknowledgements\n\nWe would like to thank Posit, PBC, for support in the development of the\n`rsample` and `spatialsample` packages.\n\n# Software, data, and code availability\n\nThe `spatialsample` package is available online at\nhttps://github.com/tidymodels/spatialsample . All data and code used in this\npaper are available online at https://github.com/cafri-labs/assessing-spatial-cv\n.\n\n\n\n\n{{< pagebreak >}}\n\n\n\n\n\n\n\n# References {#references .unnumbered}\n",
    "supporting": [
      "paper_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"paper_files/libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"paper_files/libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}